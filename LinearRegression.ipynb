{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Regression, you work with a dataset in the form of matrix $X (n \\times p)$ where\n",
    "+ $n$ represents the number of data points in the dataset (row)\n",
    "+ $p$ represents the number of features or covariates in the dataset (columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression_GD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = None  # Store the trained coefficients\n",
    "\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        # Reshape y to ensure it is a column vector (n_samples, 1)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        # Initialize beta with random values (for n features + bias)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.beta = np.random.rand(n_features + 1, 1)  # Including bias term\n",
    "        \n",
    "        # Add bias term (column of ones) to the input features\n",
    "        X = np.c_[np.ones(n_samples), X]\n",
    "\n",
    "        # Gradient descent loop\n",
    "        for i in range(epochs):\n",
    "            # Compute the gradient of the loss function (MSE)\n",
    "            error = X @ self.beta - y  # Prediction error\n",
    "            gradient = (2 / n_samples) * (X.T @ error)\n",
    "            \n",
    "            # Update beta using gradient descent\n",
    "            self.beta -= self.learning_rate * gradient\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            loss = np.mean(error ** 2)\n",
    "            print(f\"Epoch: {i+1}, Beta: {self.beta.flatten()}, Loss: {loss:.6f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add bias term (column of ones) to the input features\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X @ self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-16914.26306559, -16588.45208308, -14382.16455375])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(100, 3) * 10  \n",
    "y = 3*X[:,0] + 2*X[:,1] + X[:,2] + np.random.normal(0, 1, 100)  # Linear relationship with noise\n",
    "\n",
    "def derivative_1(X,y):\n",
    "    beta = [1,1,1]\n",
    "    return 2 * X.T @ ((X @ beta) - y)\n",
    "\n",
    "derivative_1(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Beta: [0.47231359 2.78278079 3.1682895  2.53872797], Loss: 467.653009\n",
      "Epoch: 2, Beta: [0.2037124  1.56534149 1.53283948 0.95250821], Loss: 214.964668\n",
      "Epoch: 3, Beta: [0.3838114  2.65164976 2.53992877 1.8536545 ], Loss: 100.077393\n",
      "Epoch: 4, Beta: [0.26293366 2.14450285 1.79598442 1.11465066], Loss: 47.530494\n",
      "Epoch: 5, Beta: [0.34344768 2.66436386 2.24076158 1.49664098], Loss: 23.289031\n",
      "Epoch: 6, Beta: [0.28880329 2.46296054 1.90129022 1.14591595], Loss: 11.968168\n",
      "Epoch: 7, Beta: [0.32454064 2.71780827 2.09692618 1.30147224], Loss: 6.590776\n",
      "Epoch: 8, Beta: [0.29957521 2.64492579 1.94159841 1.13061649], Loss: 3.977566\n",
      "Epoch: 9, Beta: [0.31516563 2.77366825 2.02735877 1.18917303], Loss: 2.669789\n",
      "Epoch: 10, Beta: [0.30348812 2.75266504 1.95621227 1.10297129], Loss: 1.991483\n",
      "Epoch: 11, Beta: [0.31000482 2.82004479 1.99379493 1.12125665], Loss: 1.625050\n",
      "Epoch: 12, Beta: [0.30426962 2.81845536 1.96130705 1.0758103 ], Loss: 1.418407\n",
      "Epoch: 13, Beta: [0.30669437 2.85512159 1.97790645 1.07832814], Loss: 1.296889\n",
      "Epoch: 14, Beta: [0.30361233 2.85966272 1.96324827 1.0531226 ], Loss: 1.222668\n",
      "Epoch: 15, Beta: [0.30418858 2.88042892 1.97076966 1.05019788], Loss: 1.175853\n",
      "Epoch: 16, Beta: [0.30228875 2.88599717 1.9643568  1.03545706], Loss: 1.145548\n",
      "Epoch: 17, Beta: [0.3020283  2.89821545 1.96796645 1.03125008], Loss: 1.125526\n",
      "Epoch: 18, Beta: [0.30065404 2.90309246 1.96536077 1.02218644], Loss: 1.112089\n",
      "Epoch: 19, Beta: [0.30001417 2.91053108 1.96727637 1.01823006], Loss: 1.102957\n",
      "Epoch: 20, Beta: [0.29887317 2.91432723 1.96640997 1.01241279], Loss: 1.096687\n",
      "Epoch: 21, Beta: [0.29806117 2.91898974 1.96757201 1.00915838], Loss: 1.092345\n",
      "Epoch: 22, Beta: [0.29702383 2.92178485 1.96747703 1.0052969 ], Loss: 1.089311\n",
      "Epoch: 23, Beta: [0.29613408 2.92477883 1.96828016 1.00277923], Loss: 1.087174\n",
      "Epoch: 24, Beta: [0.29514328 2.92677881 1.96850363 1.00015262], Loss: 1.085654\n",
      "Epoch: 25, Beta: [0.29421904 2.92874054 1.96911341 0.99826792], Loss: 1.084561\n",
      "Epoch: 26, Beta: [0.29324986 2.93015152 1.96944542 0.9964518 ], Loss: 1.083765\n",
      "Epoch: 27, Beta: [0.2923111  2.93145965 1.96993335 0.9950684 ], Loss: 1.083177\n",
      "Epoch: 28, Beta: [0.29135281 2.93245024 1.97027952 0.99380034], Loss: 1.082733\n",
      "Epoch: 29, Beta: [0.29040888 2.93333699 1.9706794  0.99279831], Loss: 1.082392\n",
      "Epoch: 30, Beta: [0.28945696 2.93403384 1.97100033 0.99190904], Loss: 1.082124\n",
      "Epoch: 31, Beta: [0.28851232 2.93464519 1.97133114 0.991191  ], Loss: 1.081906\n",
      "Epoch: 32, Beta: [0.28756495 2.93513934 1.9716134  0.99056761], Loss: 1.081724\n",
      "Epoch: 33, Beta: [0.28662173 2.93556884 1.9718881  0.99005855], Loss: 1.081569\n",
      "Epoch: 34, Beta: [0.28567821 2.93592412 1.97213014 0.98962385], Loss: 1.081432\n",
      "Epoch: 35, Beta: [0.2847375  2.93623258 1.97235896 0.98926758], Loss: 1.081308\n",
      "Epoch: 36, Beta: [0.2837976  2.93649308 1.97256428 0.98896779], Loss: 1.081194\n",
      "Epoch: 37, Beta: [0.28285993 2.93672049 1.97275577 0.98872287], Loss: 1.081088\n",
      "Epoch: 38, Beta: [0.2819236  2.93691641 1.97292966 0.98852005], Loss: 1.080986\n",
      "Epoch: 39, Beta: [0.28098926 2.93708925 1.97309106 0.98835615], Loss: 1.080888\n",
      "Epoch: 40, Beta: [0.28005652 2.93724117 1.97323901 0.98822331], Loss: 1.080793\n",
      "Epoch: 41, Beta: [0.27912565 2.93737712 1.97337643 0.98811831], Loss: 1.080700\n",
      "Epoch: 42, Beta: [0.27819651 2.93749903 1.97350346 0.9880361 ], Loss: 1.080609\n",
      "Epoch: 43, Beta: [0.27726921 2.93760992 1.97362192 0.98797393], Loss: 1.080519\n",
      "Epoch: 44, Beta: [0.27634368 2.93771131 1.97373235 0.98792842], Loss: 1.080431\n",
      "Epoch: 45, Beta: [0.27541998 2.93780511 1.97383597 0.9878974 ], Loss: 1.080343\n",
      "Epoch: 46, Beta: [0.27449808 2.93789243 1.97393338 0.98787854], Loss: 1.080256\n",
      "Epoch: 47, Beta: [0.273578   2.93797451 1.97402544 0.98787017], Loss: 1.080169\n",
      "Epoch: 48, Beta: [0.27265973 2.93805215 1.9741127  0.98787066], Loss: 1.080083\n",
      "Epoch: 49, Beta: [0.27174329 2.93812615 1.97419582 0.98787877], Loss: 1.079997\n",
      "Epoch: 50, Beta: [0.27082865 2.93819708 1.97427524 0.98789332], Loss: 1.079912\n",
      "Epoch: 51, Beta: [0.26991583 2.93826547 1.97435144 0.98791341], Loss: 1.079827\n",
      "Epoch: 52, Beta: [0.26900482 2.93833173 1.97442481 0.98793818], Loss: 1.079743\n",
      "Epoch: 53, Beta: [0.26809563 2.9383962  1.97449571 0.98796696], Loss: 1.079659\n",
      "Epoch: 54, Beta: [0.26718824 2.93845917 1.97456443 0.98799915], Loss: 1.079576\n",
      "Epoch: 55, Beta: [0.26628265 2.93852087 1.97463124 0.98803424], Loss: 1.079492\n",
      "Epoch: 56, Beta: [0.26537887 2.9385815  1.97469638 0.9880718 ], Loss: 1.079410\n",
      "Epoch: 57, Beta: [0.26447689 2.93864122 1.97476005 0.98811145], Loss: 1.079327\n",
      "Epoch: 58, Beta: [0.2635767  2.93870017 1.97482243 0.98815289], Loss: 1.079245\n",
      "Epoch: 59, Beta: [0.26267831 2.93875845 1.97488368 0.98819583], Loss: 1.079163\n",
      "Epoch: 60, Beta: [0.26178171 2.93881615 1.97494393 0.98824005], Loss: 1.079081\n",
      "Epoch: 61, Beta: [0.2608869  2.93887336 1.9750033  0.98828535], Loss: 1.079000\n",
      "Epoch: 62, Beta: [0.25999387 2.93893014 1.9750619  0.98833156], Loss: 1.078919\n",
      "Epoch: 63, Beta: [0.25910262 2.93898654 1.97511982 0.98837854], Loss: 1.078839\n",
      "Epoch: 64, Beta: [0.25821314 2.9390426  1.97517712 0.98842615], Loss: 1.078759\n",
      "Epoch: 65, Beta: [0.25732545 2.93909836 1.97523389 0.9884743 ], Loss: 1.078679\n",
      "Epoch: 66, Beta: [0.25643952 2.93915386 1.97529018 0.9885229 ], Loss: 1.078599\n",
      "Epoch: 67, Beta: [0.25555536 2.93920912 1.97534604 0.98857185], Loss: 1.078520\n",
      "Epoch: 68, Beta: [0.25467296 2.93926415 1.97540151 0.98862111], Loss: 1.078441\n",
      "Epoch: 69, Beta: [0.25379232 2.93931899 1.97545663 0.98867061], Loss: 1.078362\n",
      "Epoch: 70, Beta: [0.25291344 2.93937364 1.97551144 0.9887203 ], Loss: 1.078284\n",
      "Epoch: 71, Beta: [0.25203631 2.93942811 1.97556596 0.98877014], Loss: 1.078206\n",
      "Epoch: 72, Beta: [0.25116093 2.93948242 1.97562022 0.98882009], Loss: 1.078128\n",
      "Epoch: 73, Beta: [0.25028729 2.93953658 1.97567424 0.98887013], Loss: 1.078051\n",
      "Epoch: 74, Beta: [0.2494154  2.93959059 1.97572804 0.98892023], Loss: 1.077974\n",
      "Epoch: 75, Beta: [0.24854525 2.93964446 1.97578163 0.98897036], Loss: 1.077897\n",
      "Epoch: 76, Beta: [0.24767683 2.93969819 1.97583504 0.98902051], Loss: 1.077821\n",
      "Epoch: 77, Beta: [0.24681015 2.9397518  1.97588826 0.98907066], Loss: 1.077744\n",
      "Epoch: 78, Beta: [0.24594519 2.93980527 1.97594131 0.98912079], Loss: 1.077668\n",
      "Epoch: 79, Beta: [0.24508196 2.93985863 1.97599421 0.98917089], Loss: 1.077593\n",
      "Epoch: 80, Beta: [0.24422045 2.93991186 1.97604695 0.98922096], Loss: 1.077518\n",
      "Epoch: 81, Beta: [0.24336066 2.93996498 1.97609955 0.98927099], Loss: 1.077443\n",
      "Epoch: 82, Beta: [0.24250259 2.94001798 1.97615201 0.98932095], Loss: 1.077368\n",
      "Epoch: 83, Beta: [0.24164622 2.94007086 1.97620433 0.98937086], Loss: 1.077294\n",
      "Epoch: 84, Beta: [0.24079157 2.94012364 1.97625653 0.98942071], Loss: 1.077220\n",
      "Epoch: 85, Beta: [0.23993862 2.9401763  1.97630859 0.98947048], Loss: 1.077146\n",
      "Epoch: 86, Beta: [0.23908737 2.94022885 1.97636054 0.98952018], Loss: 1.077072\n",
      "Epoch: 87, Beta: [0.23823781 2.94028129 1.97641236 0.9895698 ], Loss: 1.076999\n",
      "Epoch: 88, Beta: [0.23738996 2.94033362 1.97646407 0.98961934], Loss: 1.076926\n",
      "Epoch: 89, Beta: [0.23654379 2.94038585 1.97651566 0.9896688 ], Loss: 1.076854\n",
      "Epoch: 90, Beta: [0.23569931 2.94043797 1.97656714 0.98971817], Loss: 1.076781\n",
      "Epoch: 91, Beta: [0.23485652 2.94048998 1.97661851 0.98976746], Loss: 1.076709\n",
      "Epoch: 92, Beta: [0.23401541 2.94054189 1.97666977 0.98981666], Loss: 1.076638\n",
      "Epoch: 93, Beta: [0.23317597 2.94059369 1.97672092 0.98986577], Loss: 1.076566\n",
      "Epoch: 94, Beta: [0.23233821 2.94064539 1.97677196 0.98991479], Loss: 1.076495\n",
      "Epoch: 95, Beta: [0.23150212 2.94069698 1.97682289 0.98996371], Loss: 1.076424\n",
      "Epoch: 96, Beta: [0.2306677  2.94074847 1.97687372 0.99001255], Loss: 1.076354\n",
      "Epoch: 97, Beta: [0.22983494 2.94079986 1.97692444 0.99006129], Loss: 1.076283\n",
      "Epoch: 98, Beta: [0.22900385 2.94085114 1.97697506 0.99010994], Loss: 1.076213\n",
      "Epoch: 99, Beta: [0.22817441 2.94090232 1.97702558 0.99015849], Loss: 1.076143\n",
      "Epoch: 100, Beta: [0.22734663 2.94095339 1.97707599 0.99020695], Loss: 1.076074\n"
     ]
    }
   ],
   "source": [
    "model_GD = LinearRegression_GD()\n",
    "model_GD.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
